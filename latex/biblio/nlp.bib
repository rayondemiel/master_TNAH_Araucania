
@article{alvarez-melladoTEIfriendlyAnnotationScheme2021,
  title = {{{TEI-friendly}} Annotation Scheme for Medieval Named Entities: A Case on a {{Spanish}} Medieval Corpus},
  shorttitle = {{{TEI-friendly}} Annotation Scheme for Medieval Named Entities},
  author = {Álvarez-Mellado, Elena and Díez-Platas, María Luisa and Ruiz-Fabo, Pablo and Bermúdez, Helena and Ros, Salvador and González-Blanco, Elena},
  date = {2021-06-01},
  journaltitle = {Language Resources and Evaluation},
  shortjournal = {Lang Resources \& Evaluation},
  volume = {55},
  number = {2},
  pages = {525--549},
  issn = {1574-0218},
  doi = {10.1007/s10579-020-09516-2},
  url = {https://doi.org/10.1007/s10579-020-09516-2},
  urldate = {2022-08-06},
  abstract = {Medieval documents are a rich source of historical data. Performing named-entity recognition (NER) on this genre of texts can provide us with valuable historical evidence. However, traditional NER categories and schemes are usually designed with modern documents in mind (i.e. journalistic text) and the general-domain NER annotation schemes fail to capture the nature of medieval entities. In this paper we explore the challenges of performing named-entity annotation on a corpus of Spanish medieval documents: we discuss the mismatches that arise when applying traditional NER categories to a corpus of Spanish medieval documents and we propose a novel humanist-friendly TEI-compliant annotation scheme and guidelines intended to capture the particular nature of medieval entities.},
  langid = {english},
  keywords = {Annotation scheme,Historical NER,Medieval named entities,Medieval Spanish corpus,Named-entity annotation,nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\Z84BB7GQ\\Álvarez-Mellado et al. - 2021 - TEI-friendly annotation scheme for medieval named .pdf}
}

@software{barrusPyspellchecker2022,
  title = {Pyspellchecker},
  author = {Barrus, Tyler},
  date = {2022-08-29T15:39:29Z},
  origdate = {2018-02-24T01:21:50Z},
  url = {https://github.com/barrust/pyspellchecker},
  urldate = {2022-08-30},
  abstract = {Pure Python Spell Checking http://pyspellchecker.readthedocs.io/en/latest/},
  version = {0.6.3},
  keywords = {levenshtein-distance,nlp,python,python-spell-checking,spellcheck,spellchecker,spelling-checker}
}

@misc{caneteSpanishPreTrainedBERT2020,
  title = {Spanish {{Pre-Trained BERT Model}} and {{Evaluation Data}}},
  author = {Canete, José and Chaperon and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
  date = {2020},
  publisher = {{PML4DC at ICLR 2020}},
  location = {{Santiago (Chili)}},
  url = {https://github.com/dccuchile/beto},
  keywords = {nlp}
}

@article{chaabiAmazighSpellChecker2022,
  title = {Amazigh Spell Checker Using {{Damerau-Levenshtein}} Algorithm and {{N-gram}}},
  author = {Chaabi, Youness and Ataa Allah, Fadoua},
  date = {2022-09-01},
  journaltitle = {Journal of King Saud University - Computer and Information Sciences},
  shortjournal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {34},
  pages = {6116--6124},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2021.07.015},
  url = {https://www.sciencedirect.com/science/article/pii/S1319157821001828},
  urldate = {2022-08-31},
  abstract = {Natural language processing (NLP) is a rapidly growing research field in computer science and cognitive science. Automatic correction of lexical errors is one of the applications of NLP, which aims to detect errors in the text and suggest possible corrections based on computer and linguistic models. After a state of the art and a comparison of spelling correction approaches, we developed a spelling error correction system for the Amazigh language combining the Damerau-Levenshtein algorithm and N-gram. This tool will propose possible corrections for each misspelled word in the text. Successful tests have been carried out using an Amazigh corpus.},
  issue = {8, Part B},
  langid = {english},
  keywords = {Amazigh Language,Levenshtein algorithm,Lexical errors,N-gram,nlp,Spellchecking},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\AWU3ECL8\\Chaabi et Ataa Allah - 2022 - Amazigh spell checker using Damerau-Levenshtein al.pdf;C\:\\Users\\cohum\\Zotero\\storage\\VYQ2YYTE\\S1319157821001828.html}
}

@thesis{clericeDetectionIsotopiesPar2022,
  type = {thèse de doctorat en lettres et civilisations antiques, dir. Christian Nicolas},
  title = {Détection d'isotopies Par Apprentissage Profond : L'exemple de La Sexualité En Latin Classique et Tardif},
  author = {Clérice, Thibault},
  date = {2022-03-28},
  institution = {{Université Lyon 3}},
  location = {{Lyon}},
  url = {https://github.com/PonteIneptique/these-redaction/releases/tag/1.0.1},
  keywords = {nlp}
}

@misc{devlinBERTPretrainingDeep2019a,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2022-09-06},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\RMFLAV56\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;C\:\\Users\\cohum\\Zotero\\storage\\IPB5R2GJ\\1810.html}
}

@article{ehrmannEntiteesNommeesLinguistique,
  title = {Les {{Entitées Nommées}}, de La Linguistique Au {{TAL}}: {{Statut}} Théorique et Méthodes de Désambiguïsation},
  author = {Ehrmann, Maud},
  pages = {296},
  langid = {english},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\R4JFFHCB\\Ehrmann - Les Entitées Nommées, de la linguistique au TAL S.pdf}
}

@misc{gabayManuelAnnotationLinguistique2022,
  title = {Manuel d'annotation Linguistique Pour Le Français Moderne ({{XVIe}} -{{XVIIIe}} Siècles)},
  author = {Gabay, Simon and Camps, Jean-Baptiste and Clérice, Thibault},
  date = {2022-04},
  url = {https://hal.archives-ouvertes.fr/hal-02571190},
  urldate = {2022-09-07},
  keywords = {Annotation de corpus,Corpus annotation and tagging,Etiquetage morpho-syntaxique,Français moderne,Lemmatisation,Modern French,Named entity recognition,nlp,POS annotation,Reconnaissance d'entités nommées},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\G7KRFKMW\\Gabay et al. - 2022 - Manuel d'annotation linguistique pour le français .pdf}
}

@article{gaikwadEffectiveEfficientXML2013,
  title = {Effective and {{Efficient XML Duplicate Detection Using Levenshtein Distance Algorithm}}},
  author = {Gaikwad, Shital and Bogiri, Nagaraju},
  date = {2013},
  volume = {4},
  number = {6},
  pages = {5},
  abstract = {There is big amount of work on discovering duplicates in relational data; merely elite findings concentrate on duplication in additional multifaceted hierarchical structures. Electronic information is one of the key factors in several business operations, applications, and determinations, at the same time as an outcome, guarantee its superiority is necessary. Duplicates are several delegacy of the identical real world thing which is dissimilar from each other. Duplicate finding a little assignment because of the actuality that duplicates are not accurately equivalent, frequently because of the errors in the information. Accordingly, many data processing techniques never apply widespread assessment algorithms which identify precise duplicates. As an alternative, evaluate all objective representations, by means of a probably compound identical approach, to identifying that the object is real world or not. Duplicate detection is applicable in data clean-up and data incorporation applications and which considered comprehensively for relational data or XML document.This paper it is suggested to use Levenshtein distance algorithm which is best and efficient than the previous Normalized Edit Distance (NED) algorithm. This paper will provide the person who reads with the groundwork for research in Duplicate Detection in XML data or Hierarchical Data.},
  langid = {english},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\FREBAX8C\\Gaikwad et Bogiri - 2013 - Effective and Efficient XML Duplicate Detection Us.pdf}
}

@inproceedings{gaikwadLevenshteinDistanceAlgorithm2015,
  title = {Levenshtein Distance Algorithm for Efficient and Effective {{XML}} Duplicate Detection},
  author = {Gaikwad, Shital and Nagaraju, Bogiri},
  date = {2015-09-01},
  pages = {1--5},
  doi = {10.1109/IC4.2015.7375698},
  abstract = {Electronic Data Processing used automated methods for processing commercial data. There is big amount of work on discovering duplicates in relational data; merely elite findings concentrate on duplication in additional multifaceted hierarchical structures Electronic information is one of the key factors in several business operations, applications, and determinations, at the same time as an outcome, guarantee its superiority is necessary. Duplicate finding a little assignment because of the actuality that duplicates are not accurately equivalent, frequently because of the errors in the information. Accordingly, many data processing techniques never apply widespread assessment algorithms which identify precise duplicates. As an alternative, evaluate all objective representations, by means of a probably compound identical approach, to identifying that the object is real world or not. Duplicate detection is applicable in data clean-up and data incorporation applications and which considered comprehensively for relational data or XML document. This paper will provide the person who reads with the groundwork for research in Efficient and Effective Duplicate Detection in Hierarchical Data or XML data.},
  keywords = {nlp}
}

@article{haldarLevenshteinDistanceTechnique,
  title = {Levenshtein {{Distance Technique}} in {{Dictionary Lookup Methods}}: {{An Improved Approach}}},
  author = {Haldar, Rishin and Mukhopadhyay, Debajyoti},
  pages = {5},
  doi = {10.48550},
  url = {https://arxiv.org/abs/1101.1232},
  abstract = {Dictionary lookup methods are popular in dealing with ambiguous letters which were not recognized by Optical Character Readers. However, a robust dictionary lookup method can be complex as apriori probability calculation or a large dictionary size increases the overhead and the cost of searching. In this context, Levenshtein distance is a simple metric which can be an effective string approximation tool. After observing the effectiveness of this method, an improvement has been made to this method by grouping some similar looking alphabets and reducing the weighted difference among members of the same group. The results showed marked improvement over the traditional Levenshtein distance technique.},
  langid = {english},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\8NCKKXAZ\\Haldar et Mukhopadhyay - Levenshtein Distance Technique in Dictionary Looku.pdf}
}

@article{karthikeyanOCRPostCorrectionApproach2022,
  title = {An {{OCR Post-Correction Approach Using Deep Learning}} for {{Processing Medical Reports}}},
  author = {Karthikeyan, Srinidhi and de Herrera, Alba G. Seco and Doctor, Faiyaz and Mirza, Asim},
  options = {useprefix=true},
  date = {2022-05},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {32},
  number = {5},
  pages = {2574--2581},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2021.3087641},
  abstract = {According to a recent Deloitte study, the COVID-19 pandemic continues to place a huge strain on the global health care sector. Covid-19 has also catalysed digital transformation across the sector for improving operational efficiencies. As a result, the amount of digitally stored patient data such as discharge letters, scan images, test results or free text entries by doctors has grown significantly. In 2020, 2314 exabytes of medical data was generated globally. This medical data does not conform to a generic structure and is mostly in the form of unstructured digitally generated or scanned paper documents stored as part of a patient’s medical reports. This unstructured data is digitised using Optical Character Recognition (OCR) process. A key challenge here is that the accuracy of the OCR process varies due to the inability of current OCR engines to correctly transcribe scanned or handwritten documents in which text may be skewed, obscured or illegible. This is compounded by the fact that processed text is comprised of specific medical terminologies that do not necessarily form part of general language lexicons. The proposed work uses a deep neural network based self-supervised pre-training technique: Robustly Optimized Bidirectional Encoder Representations from Transformers (RoBERTa) that can learn to predict hidden (masked) sections of text to fill in the gaps of non-transcribable parts of the documents being processed. Evaluating the proposed method on domain-specific datasets which include real medical documents, shows a significantly reduced word error rate demonstrating the effectiveness of the approach.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  keywords = {Biomedical imaging,Engines,Medical diagnostic imaging,medical documents,Medical services,natural language processing (NLP),nlp,Optical character recognition (OCR),Optical character recognition software,Portable document format,robustly optimized bidirectional encoder representations from transformers (RoBERTa),Task analysis},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\8H7QN2KV\\Karthikeyan et al. - 2022 - An OCR Post-Correction Approach Using Deep Learnin.pdf;C\:\\Users\\cohum\\Zotero\\storage\\ZALRIJVZ\\9448197.html}
}

@inproceedings{kissosOCRErrorCorrection2016,
  title = {{{OCR Error Correction Using Character Correction}} and {{Feature-Based Word Classification}}},
  booktitle = {2016 12th {{IAPR Workshop}} on {{Document Analysis Systems}} ({{DAS}})},
  author = {Kissos, Ido and Dershowitz, Nachum},
  date = {2016-04},
  pages = {198--203},
  publisher = {{IEEE}},
  location = {{Santorini, Greece}},
  doi = {10.1109/DAS.2016.44},
  url = {http://ieeexplore.ieee.org/document/7490117/},
  urldate = {2022-08-30},
  abstract = {This paper explores the use of a learned classifier for post-OCR text correction. Experiments with the Arabic language show that this approach, which integrates a weighted confusion matrix and a shallow language model, improves the vast majority of segmentation and recognition errors, the most frequent types of error on our dataset.},
  eventtitle = {2016 12th {{IAPR Workshop}} on {{Document Analysis Systems}} ({{DAS}})},
  isbn = {978-1-5090-1792-8},
  langid = {english},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\22F5L97X\\Kissos et Dershowitz - 2016 - OCR Error Correction Using Character Correction an.pdf}
}

@article{labbeNormalisationLemmatisationQuestion,
  title = {Normalisation et lemmatisation d'une question ouverte},
  author = {Labbé, Dominique},
  pages = {21},
  langid = {french},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\WIWKL7NG\\Labbé - Normalisation et lemmatisation d'une question ouve.pdf}
}

@article{loprestiOpticalCharacterRecognition2009,
  title = {Optical Character Recognition Errors and Their Effects on Natural Language Processing},
  author = {Lopresti, Daniel},
  date = {2009-09-01},
  journaltitle = {International Journal on Document Analysis and Recognition (IJDAR)},
  shortjournal = {IJDAR},
  volume = {12},
  number = {3},
  pages = {141--151},
  issn = {1433-2825},
  doi = {10.1007/s10032-009-0094-8},
  url = {https://doi.org/10.1007/s10032-009-0094-8},
  urldate = {2022-08-31},
  abstract = {Errors are unavoidable in advanced computer vision applications such as optical character recognition, and the noise induced by these errors presents a serious challenge to downstream processes that attempt to make use of such data. In this paper, we apply a new paradigm we have proposed for measuring the impact of recognition errors on the stages of a standard text analysis pipeline: sentence boundary detection, tokenization, and part-of-speech tagging. Our methodology formulates error classification as an optimization problem solvable using a hierarchical dynamic programming approach. Errors and their cascading effects are isolated and analyzed as they travel through the pipeline. We present experimental results based on a large collection of scanned pages to study the varying impact depending on the nature of the error and the character(s) involved. This dataset has also been made available online to encourage future investigations.},
  langid = {english},
  keywords = {nlp,Optical character recognition,Part-of-speech tagging,Performance evaluation,Sentence boundary detection,Tokenization},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\6DYMKN35\\Lopresti - 2009 - Optical character recognition errors and their eff.pdf}
}

@online{moodyStopUsingWord2vec2017,
  title = {Stop {{Using}} Word2vec | {{Stitch Fix Technology}} – {{Multithreaded}}},
  author = {Moody, Chris},
  date = {2017-10-18},
  url = {https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/},
  urldate = {2022-09-06},
  abstract = {When I started playing with word2vec four years ago I needed (and luckily had) tons of supercomputer time. But because of advances in our understanding of w...},
  organization = {{multithreaded}},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\7RK66DUB\\stop-using-word2vec.html}
}

@inproceedings{nguyenDeepStatisticalAnalysis2019,
  title = {Deep {{Statistical Analysis}} of {{OCR Errors}} for {{Effective Post-OCR Processing}}},
  booktitle = {2019 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Nguyen, Thi-Tuyet-Hai and Jatowt, Adam and Coustaty, Mickael and Nguyen, Nhu-Van and Doucet, Antoine},
  date = {2019-06},
  pages = {29--38},
  doi = {10.1109/JCDL.2019.00015},
  abstract = {Post-OCR is an important processing step that follows optical character recognition (OCR) and is meant to improve the quality of OCR documents by detecting and correcting residual errors. This paper describes the results of a statistical analysis of OCR errors on four document collections. Five aspects related to general OCR errors are studied and compared with human-generated misspellings, including edit operations, length effects, erroneous character positions, real-word vs. non-word errors, and word boundaries. Based on the observations from the analysis we give several suggestions related to the design and implementation of effective OCR post-processing approaches.},
  eventtitle = {2019 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  keywords = {Character recognition,Error analysis,Feature extraction,Libraries,nlp,OCR errors; OCR post-processing; post OCR text correction,Optical character recognition software,Statistical analysis,Transducers},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\W77FHHLL\\Nguyen et al. - 2019 - Deep Statistical Analysis of OCR Errors for Effect.pdf;C\:\\Users\\cohum\\Zotero\\storage\\7GNBQTN5\\8791206.html}
}

@online{norvigHowWriteSpelling2007,
  title = {How to {{Write}} a {{Spelling Corrector}}},
  author = {Norvig, Peter},
  date = {2007-02},
  url = {https://norvig.com/spell-correct.html},
  urldate = {2022-08-31},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\WQJZJGNT\\spell-correct.html}
}

@misc{palVartaniSpellcheckAutomatic2020,
  title = {Vartani {{Spellcheck}} -- {{Automatic Context-Sensitive Spelling Correction}} of {{OCR-generated Hindi Text Using BERT}} and {{Levenshtein Distance}}},
  author = {Pal, Aditya and Mustafi, Abhijit},
  date = {2020-12-14},
  number = {arXiv:2012.07652},
  eprint = {2012.07652},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.07652},
  url = {http://arxiv.org/abs/2012.07652},
  urldate = {2022-08-31},
  abstract = {Traditional Optical Character Recognition (OCR) systems that generate text of highly inflectional Indic languages like Hindi tend to suffer from poor accuracy due to a wide alphabet set, compound characters and difficulty in segmenting characters in a word. Automatic spelling error detection and context-sensitive error correction can be used to improve accuracy by post-processing the text generated by these OCR systems. A majority of previously developed language models for error correction of Hindi spelling have been context-free. In this paper, we present Vartani Spellcheck - a context-sensitive approach for spelling correction of Hindi text using a state-of-the-art transformer - BERT in conjunction with the Levenshtein distance algorithm, popularly known as Edit Distance. We use a lookup dictionary and context-based named entity recognition (NER) for detection of possible spelling errors in the text. Our proposed technique has been tested on a large corpus of text generated by the widely used Tesseract OCR on the Hindi epic Ramayana. With an accuracy of 81\%, the results show a significant improvement over some of the previously established context-sensitive error correction mechanisms for Hindi. We also explain how Vartani Spellcheck may be used for on-the-fly autocorrect suggestion during continuous typing in a text editor environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\QREAE9S6\\Pal et Mustafi - 2020 - Vartani Spellcheck -- Automatic Context-Sensitive .pdf;C\:\\Users\\cohum\\Zotero\\storage\\P56SXDX5\\2012.html}
}

@article{poibeauExtractionInformationNouvelle1999,
  title = {L'extraction d'information, Une Nouvelle Conception de La Compréhension de Texte ?},
  author = {Poibeau, Thierry and Nazarenko, Adeline},
  date = {1999},
  journaltitle = {Traitement Automatique des Langues},
  volume = {2},
  number = {40},
  pages = {87--115},
  keywords = {nlp}
}

@inproceedings{rigaudICDAR2019Competition2019,
  title = {{{ICDAR}} 2019 {{Competition}} on {{Post-OCR Text Correction}}},
  booktitle = {15th {{International Conference}} on {{Document Analysis}} and {{Recognition}}},
  author = {Rigaud, Christophe and Doucet, Antoine and Coustaty, Mickaël and Moreux, Jean-Philippe},
  date = {2019-09},
  pages = {1588--1593},
  location = {{Sydney, Australia}},
  url = {https://hal.archives-ouvertes.fr/hal-02304334},
  urldate = {2022-08-31},
  abstract = {This paper describes the second round of the ICDAR 2019 competition on post-OCR text correction and presents the different methods submitted by the participants. OCR has been an active research field for over the past 30 years but results are still imperfect, especially for historical documents. The purpose of this competition is to compare and evaluate automatic approaches for correcting (denoising) OCR-ed texts. The present challenge consists of two tasks: 1) error detection and 2) error correction. An original dataset of 22M OCR-ed symbols along with an aligned ground truth was provided to the participants with 80\% of the dataset dedicated to training and 20\% to evaluation. Different sources were aggregated and contain newspapers, historical printed documents as well as manuscripts and shopping receipts, covering 10 European languages (Bulgarian, Czech, Dutch, English, Finish, French, German, Polish, Spanish and Slovak). Five teams submitted results, the error detection scores vary from 41 to 95\% and the best error correction improvement is 44\%. This competition, which counted 34 registrations, illustrates the strong interest of the community to improve OCR output, which is a key issue to any digitization process involving textual data.},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\J99NKLGL\\Rigaud et al. - 2019 - ICDAR 2019 Competition on Post-OCR Text Correction.pdf}
}

@software{SpacytransformersUsePretrained2022,
  title = {Spacy-Transformers: {{Use}} Pretrained Transformers like {{BERT}}, {{XLNet}} and {{GPT-2}} in {{spaCy}}},
  shorttitle = {Spacy-Transformers},
  date = {2022-09-07T16:46:46Z},
  origdate = {2019-07-26T19:12:34Z},
  url = {https://github.com/explosion/spacy-transformers},
  urldate = {2022-09-08},
  abstract = {🛸 Use pretrained transformers like BERT, XLNet and GPT-2 in spaCy},
  organization = {{Explosion}},
  version = {1.1.8},
  keywords = {bert,google,gpt-2,huggingface,language-model,machine-learning,natural-language-processing,natural-language-understanding,nlp,openai,pytorch,pytorch-model,spacy,spacy-extension,spacy-pipeline,transfer-learning,xlnet}
}

@misc{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  date = {2019-06-05},
  number = {arXiv:1906.02243},
  eprint = {1906.02243},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.02243},
  url = {http://arxiv.org/abs/1906.02243},
  urldate = {2022-08-20},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\VLNB7ZW6\\Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf;C\:\\Users\\cohum\\Zotero\\storage\\TQZ7JRA8\\1906.html}
}

@article{takahashiSpellingCorrectionMethod1990,
  title = {A Spelling Correction Method and Its Application to an {{OCR}} System},
  author = {Takahashi, H. and Itoh, N. and Amano, T. and Yamashita, A.},
  date = {1990-01-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {23},
  number = {3},
  pages = {363--377},
  issn = {0031-3203},
  doi = {10.1016/0031-3203(90)90023-E},
  url = {https://www.sciencedirect.com/science/article/pii/003132039090023E},
  urldate = {2022-08-30},
  abstract = {This paper describes a method of spelling correction consisting of two steps: selection of candidate words, and approximate string matching between the input word and each candidate word. Each word is classified and multi-indexed according to combinations of a constant number of characters in the word. Candidate words are selected fast and accurately, regardless of error types, as long as the number of errors is below a threshold. We applied this method to the post-processing of a printed alphanumeric OCR on a personal computer, thus making our OCR more reliable and user-friendly.},
  langid = {english},
  keywords = {Character recognition,Learning,nlp,OCR,Pattern recognition,Post-processing,Spelling check,Spelling correction,Text processing},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\VKXBGYHF\\003132039090023E.html}
}

@article{tanguyEvolutionsLinguistiqueOutillee2014,
  title = {Évolutions de la linguistique outillée : méfaits et bienfaits du TAL},
  shorttitle = {Évolutions de la linguistique outillée},
  author = {Tanguy, Ludovic and Fabre, Cécile},
  date = {2014},
  journaltitle = {L'information grammaticale},
  number = {142},
  pages = {15},
  url = {https://halshs.archives-ouvertes.fr/halshs-01057493},
  urldate = {2022-09-05},
  abstract = {Cet article examine l'impact des technologies du traitement automatique des langues (TAL) sur les pratiques de la linguistique outillée, au sens de (Habert 2004). Cette linguistique qui s'est dotée de nouveaux instruments d'observation et de calcul est largement alimentée par les méthodes définies dans le champ du TAL, héritant ainsi des évolutions de cette discipline pour le meilleur et pour le pire. En effet, si la linguistique a l'opportunité d'intégrer dans son outillage des instruments plus sophistiqués basés sur des approches quantitatives et des données plus diverses et plus nombreuses, elle a néanmoins des difficultés à se les approprier et à contribuer en retour aux avancées du TAL. Nous rendons compte dans cet article de ce bilan contrasté. Nous présentons tout d'abord un rapide panorama des évolutions récentes, en évoquant notamment la façon dont le développement des méthodes quantitatives du TAL a modifié la place dévolue à la linguistique dans cette discipline. Nous exposons les problèmes que nous percevons actuellement\textasciitilde : un TAL à l'outillage trop complexe et opaque pour être appréhendé pleinement par des linguistes, des linguistes dont le rôle se cantonne à l'annotation de données d'entraînement ou d'évaluation, des applications qui simplifient la définition des objets langagiers en misant sur la massification des données pour les traiter. La deuxième partie s'intéresse à l'impact, cette fois plus positif, qu'ont eu ces évolutions majeures du TAL sur la méthodologie des recherches en linguistique. Elles se traduisent par la possibilité de faire appel à de nouvelles méthodes d'expérimentation et de reconsidérer les phénomènes langagiers à la lumière de données à grande échelle, assorties de méthodes d'observation plus efficaces. La dernière partie suggère une voie permettant à la linguistique ainsi outillée de jouer pleinement son rôle face à de nouvelles demandes exprimées par des acteurs externes (autres disciplines, entreprises, institutions, etc.). Ces demandes, de natures diverses, ne semblent en effet pas adaptées aux exigences du TAL quantitatif qui s'est concentré sur quelques applications centrales. La linguistique outillée garde alors sa spécificité et son avantage dans des situations qui requièrent des connaissances linguistiques et un traitement plus riche du matériau langagier. Dans ce genre de situation, parce qu'elle a bénéficié des avancées récentes du TAL, la linguistique est aujourd'hui à même d'intervenir efficacement sur de grandes quantités de données et de dialoguer avec d'autres disciplines et acteurs de terrain. Elle a alors l'opportunité de faire émerger des problématiques linguistiques sur des données souvent très riches, et de traiter des questions nouvelles, dont nous donnons plusieurs exemples.},
  langid = {french},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\FZS7ADKY\\Tanguy et Fabre - 2014 - Évolutions de la linguistique outillée  méfaits e.pdf;C\:\\Users\\cohum\\Zotero\\storage\\CHMP9XT9\\halshs-01057493.html}
}

@thesis{tanguyTraitementAutomatiqueLangue1997,
  type = {thèse d'informatique, dir. Jean-Pierre Barthélémy et Ioannis Kanellos},
  title = {Traitement Automatique de la Langue Naturelle et interprétation:Contribution à l'élaboration d'un modèle informatique de la Sémantique Interprétative},
  shorttitle = {Traitement Automatique de la Langue Naturelle et interprétation},
  author = {Tanguy, Ludovic},
  date = {1997-05-07},
  institution = {{Université de Rennes 1}},
  url = {https://halshs.archives-ouvertes.fr/tel-01322692},
  urldate = {2022-09-05},
  abstract = {Nous proposons dans cette thèse une modélisation informatique de la sémantique interprétative de F. Rastier. Après une première critique des traitements classiques du langage naturel sous ses aspects sémantiques, nous concluons à une non-réductibilité du sens d'un texte à une structure calculée à partir de descriptions locales, et à la nécessité de déterminations globales. Nous étudions ensuite la théorie linguistique de la sémantique interprétative, afin d'en retirer les concepts centraux captables par une approche informatique. Nous proposons un modèle formel de description de ceux-ci, ainsi que des mécanismes de manipulation des structures sémantiques descriptives. Enfin, nous proposons un logiciel appliquant cette formalisation, sous la forme d'une assistance à l'analyse des textes. Nous rejoignons une approche de coopération homme-machine, en distinguant les activités interprétatives propres à l'utilisateur de leur manipulation formelle par la machine. Nous concluons par une mise en relief des avantages suggestifs et descriptifs d'une approche informatique de certains aspects de l'interprétation.},
  langid = {french},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\68XX2P6D\\Tanguy - 1997 - Traitement Automatique de la Langue Naturelle et i.pdf;C\:\\Users\\cohum\\Zotero\\storage\\44E2E5ET\\tel-01322692.html}
}

@inproceedings{terrielAtelierProductionModele2021,
  title = {Atelier : {{Production}} d'un Modèle Affiné de Reconnaissance d'écriture Manuscrite Avec {{eScriptorium}} et Évaluation de Ses Performances. {{Évaluer}} Son Modèle {{HTR}}/{{OCR}} Avec {{KaMI}} ({{Kraken}} as {{Model Inspector}})},
  shorttitle = {Atelier},
  booktitle = {Les {{Futurs Fantastiques}} - 3e {{Conférence Internationale}} Sur l'{{Intelligence Artificielle}} Appliquée Aux {{Bibliothèques}}, {{Archives}} et {{Musées}}},
  author = {Terriel, Lucas},
  date = {2021-12},
  publisher = {{AI4LAM and Bibliothèque nationale de France}},
  location = {{Paris, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03495762},
  urldate = {2022-08-29},
  keywords = {Evaluation,Handwritten text recognition,HTR,Librairie informatique,Logiciel d'évaluation,Métriques de similarité syntaxique,nlp,OCR,Programming package,Python,Reconnaissance automatique d'écriture manuscrite,Syntaxic similarity metrics},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\VE6HQE8J\\Terriel - 2021 - Atelier  Production d'un modèle affiné de reconna.pdf}
}

@inproceedings{tongStatisticalApproachAutomatic1996,
  title = {A {{Statistical Approach}} to {{Automatic OCR Error Correction}} in {{Context}}},
  booktitle = {Fourth {{Workshop}} on {{Very Large Corpora}}},
  author = {Tong, Xiang and Evans, David A.},
  date = {1996-06},
  publisher = {{Association for Computational Linguistics}},
  location = {{Herstmonceux Castle, Sussex, UK}},
  url = {https://aclanthology.org/W96-0108},
  urldate = {2022-08-30},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\QNYK4V6M\\Tong et Evans - 1996 - A Statistical Approach to Automatic OCR Error Corr.pdf}
}

@article{toussaintExtractionConnaissancesPartir2004,
  title = {Extraction de connaissances à partir de textes structurés},
  author = {Toussaint, Yannick},
  date = {2004},
  journaltitle = {Document numérique},
  shortjournal = {Document numérique},
  volume = {8},
  number = {3},
  pages = {11--34},
  publisher = {{Lavoisier}},
  location = {{Cachan}},
  issn = {1279-5127},
  doi = {10.3166/dn.8.3.11-34},
  url = {https://www.cairn.info/revue-document-numerique-2004-3-page-11.htm},
  urldate = {2022-09-06},
  abstract = {RésuméCet article propose un schéma général d’extraction de connaissances à partir de textes et situe la fouille de textes comme une étape particulière d’un processus complexe. Notre position est que tout processus de fouille de textes doit nécessairement exploiter un modèle de connaissances et qu’il est essentiel d’extraire des textes des informations structurées auxquelles peut être associée une sémantique. De ce fait, nous nous intéressons tout particulièrement à la structure des textes, structure devant être prise dans un sens très général qui va d’une structuration physique (hiérarchique) à une structuration cognitive ou sémantique. Nous montrons comment ces différentes dimensions du document et du texte peuvent ou pourraient être prises en compte pour que le processus dans son ensemble soit incrémental, c’est-à-dire qu’il soit initialisé avec un ensemble de connaissances réduit qui augmente au fur et à mesure des boucles de traitement.},
  langid = {french},
  keywords = {extraction d'information,extraction de connaissances à partir de textes,fouille de textes,modèle de connaissances,nlp,structure du document,traitement automatique de la langue},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\5T2LNB5Z\\revue-document-numerique-2004-3-page-11.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-09-06},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\USRN3EAF\\Vaswani et al. - 2017 - Attention Is All You Need.pdf;C\:\\Users\\cohum\\Zotero\\storage\\JGAYBYJX\\1706.html}
}

@inproceedings{whitelawUsingWebLanguage2009,
  title = {Using the Web for Language Independent Spellchecking and Autocorrection},
  booktitle = {Proceedings of the 2009 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing Volume}} 2 - {{EMNLP}} '09},
  author = {Whitelaw, Casey and Hutchinson, Ben and Chung, Grace Y. and Ellis, Gerard},
  date = {2009},
  volume = {2},
  pages = {890},
  publisher = {{Association for Computational Linguistics}},
  location = {{Singapore}},
  doi = {10.3115/1699571.1699629},
  url = {http://portal.acm.org/citation.cfm?doid=1699571.1699629},
  urldate = {2022-06-02},
  abstract = {We have designed, implemented and evaluated an end-to-end system spellchecking and autocorrection system that does not require any manually annotated training data. The World Wide Web is used as a large noisy corpus from which we infer knowledge about misspellings and word usage. This is used to build an error model and an n-gram language model. A small secondary set of news texts with artificially inserted misspellings are used to tune confidence classifiers. Because no manual annotation is required, our system can easily be instantiated for new languages. When evaluated on human typed data with real misspellings in English and German, our web-based systems outperform baselines which use candidate corrections based on hand-curated dictionaries. Our system achieves 3.8\% total error rate in English. We show similar improvements in preliminary results on artificial data for Russian and Arabic.},
  eventtitle = {The 2009 {{Conference}}},
  isbn = {978-1-932432-62-6},
  langid = {english},
  keywords = {nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\JXF3GZ74\\Whitelaw et al. - 2009 - Using the web for language independent spellchecki.pdf}
}

@article{widdowsonFirth1957Papers2007,
  title = {J.{{R}}. {{Firth}}, 1957, {{Papers}} in {{Linguistics}} 1934–51},
  author = {Widdowson, Henry},
  date = {2007},
  journaltitle = {International Journal of Applied Linguistics},
  volume = {17},
  number = {3},
  pages = {402--413},
  issn = {1473-4192},
  doi = {10.1111/j.1473-4192.2007.00164.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1473-4192.2007.00164.x},
  urldate = {2022-09-06},
  abstract = {This feature provides a critical reappraisal of a well-known book that was published some time ago in order to asses how far it is still relevant to current thinking.},
  langid = {english},
  keywords = {nlp},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1473-4192.2007.00164.x},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\E6NG57V5\\j.1473-4192.2007.00164.html}
}

@unpublished{yuePHICONImprovingGeneralization2020,
  title = {{{PHICON}}: {{Improving Generalization}} of {{Clinical Text De-identification Models}} via {{Data Augmentation}}},
  shorttitle = {{{PHICON}}},
  author = {Yue, Xiang and Zhou, Shuang},
  date = {2020-10-10},
  number = {arXiv:2010.05143},
  eprint = {2010.05143},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.05143},
  urldate = {2022-07-01},
  abstract = {De-identification is the task of identifying protected health information (PHI) in the clinical text. Existing neural de-identification models often fail to generalize to a new dataset. We propose a simple yet effective data augmentation method PHICON to alleviate the generalization issue. PHICON consists of PHI augmentation and Context augmentation, which creates augmented training corpora by replacing PHI entities with named-entities sampled from external sources, and by changing background context with synonym replacement or random word insertion, respectively. Experimental results on the i2b2 2006 and 2014 de-identification challenge datasets show that PHICON can help three selected de-identification models boost F1-score (by at most 8.6\%) on cross-dataset test setting. We also discuss how much augmentation to use and how each augmentation method influences the performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,nlp},
  file = {C\:\\Users\\cohum\\Zotero\\storage\\QVZCMDRB\\Yue et Zhou - 2020 - PHICON Improving Generalization of Clinical Text .pdf}
}


